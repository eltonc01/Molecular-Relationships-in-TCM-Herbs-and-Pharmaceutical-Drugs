{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021c5668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, recall_score, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from IPython.utils import io\n",
    "import time\n",
    "import joblib\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow_addons as tfa\n",
    "import ast\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import DataStructs\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e061a2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain datasets\n",
    "\n",
    "df_combos = pd.read_csv('synthetic drugcomb combos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a72a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combos = df_combos.apply(lambda row: [row['Drug 1']] + [row['Drug 2']], axis=1).to_list()\n",
    "combos = list(map(list, set(map(frozenset, combos))))\n",
    "combos = list(map(list, set(map(tuple, map(set, combos)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57e94d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop = []\n",
    "for i in combos:\n",
    "    if len(i) == 1:\n",
    "        drop.append(i)\n",
    "        \n",
    "combos = [x for x in combos if x not in drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131111a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "drugspace = []\n",
    "for i in combos:\n",
    "    if i[0] not in drugspace:\n",
    "        drugspace.append(i[0])\n",
    "    if i[1] not in drugspace:\n",
    "        drugspace.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfd4b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_drugs = drugspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d4d7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors = pd.read_csv('drugcomb db drug descriptors.csv')\n",
    "var = VarianceThreshold(0.02)\n",
    "var.fit_transform(descriptors.drop(columns='IDs')).shape[1]\n",
    "cols = var.get_feature_names_out(input_features=descriptors.drop(columns='IDs').columns)\n",
    "\n",
    "drop = []\n",
    "for i in descriptors.drop(columns='IDs').columns:\n",
    "    if i not in cols:\n",
    "        drop.append(i)\n",
    "        \n",
    "descriptors = descriptors.drop(columns=drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d891214c",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = pd.read_csv('drugcomb targets.csv')\n",
    "var = VarianceThreshold(0.02)\n",
    "var.fit_transform(targets.drop(columns='IDs')).shape[1]\n",
    "cols = var.get_feature_names_out(input_features=targets.drop(columns='IDs').columns)\n",
    "\n",
    "drop = []\n",
    "for i in targets.drop(columns='IDs').columns:\n",
    "    if i not in cols:\n",
    "        drop.append(i)\n",
    "targets = targets.drop(columns=drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8900de9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = descriptors.columns.to_list()[1:] + targets.columns.to_list()[1:]\n",
    "temp = [x + ' - 1' for x in col_names]\n",
    "col_names = col_names + temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc093c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_dict = {}\n",
    "for r in range(0, len(descriptors)):\n",
    "    index_dict[descriptors.loc[r, 'IDs']] = r\n",
    "    \n",
    "tar_dict = {}\n",
    "for r in range(0, len(targets)):\n",
    "    tar_dict[targets.loc[r, 'IDs']] = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fc7073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49030bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set validation dataset\n",
    "\n",
    "val_drugspace = random.sample(all_drugs, 500)\n",
    "drugspace = val_drugspace\n",
    "val_drugspace = drugspace\n",
    "\n",
    "combo_dict = {}\n",
    "for i in drugspace:\n",
    "    combo_dict[i] = []\n",
    "for i in combos:\n",
    "    try:\n",
    "        x = combo_dict[i[0]]\n",
    "        x = combo_dict[i[1]]\n",
    "        combo_dict[i[0]].append(i[1])\n",
    "        combo_dict[i[1]].append(i[0])\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print('setting positive combos')\n",
    "existing_combinations = []\n",
    "for i in drugspace:\n",
    "    ingredients = combo_dict[i]\n",
    "    for i1 in ingredients:\n",
    "        existing_combinations.append([i, i1])\n",
    "        \n",
    "existing_combinations = list(map(list, set(map(frozenset, existing_combinations))))\n",
    "\n",
    "n_combos = []\n",
    "count = len(existing_combinations)\n",
    "while len(n_combos) < count:\n",
    "    ingredient1 = 0\n",
    "    ingredient2 = 0\n",
    "    while ingredient1 == ingredient2:\n",
    "        ingredient1 = random.choice(drugspace)\n",
    "        ingredient2 = random.choice(drugspace)\n",
    "    if ingredient2 in combo_dict[ingredient1] or ingredient1 in combo_dict[ingredient2]:\n",
    "        continue\n",
    "    if [ingredient1, ingredient2] not in n_combos and [ingredient2, ingredient1] not in n_combos:\n",
    "        n_combos.append([ingredient1, ingredient2])\n",
    "        if len(n_combos) % 1000 == 0:\n",
    "            print(len(n_combos))\n",
    "\n",
    "print('constructing dataset')\n",
    "table = []\n",
    "for r in existing_combinations:\n",
    "    if existing_combinations.index(r) % 1000 == 0:\n",
    "        print(existing_combinations.index(r))\n",
    "    try:\n",
    "        index1 = index_dict[r[0]]\n",
    "        row1 = descriptors.loc[index1].drop('IDs').to_list()\n",
    "        \n",
    "        tarindex1 = tar_dict[r[0]]\n",
    "        tarrow1 = targets.loc[tarindex1].drop('IDs').to_list()\n",
    "        \n",
    "        index2 = index_dict[r[1]]\n",
    "        row2 = descriptors.loc[index2].drop('IDs').to_list()\n",
    "        \n",
    "        tarindex2 = tar_dict[r[1]]\n",
    "        tarrow2 = targets.loc[tarindex1].drop('IDs').to_list()\n",
    "        \n",
    "        row = row1 + tarrow1 + row2 + tarrow2\n",
    "        # row = row1 + row2\n",
    "        # row = tarrow1 + tarrow2\n",
    "        table.append(row)\n",
    "    except KeyboardInterrupt:\n",
    "        raise\n",
    "    except:\n",
    "        del existing_combinations[existing_combinations.index(r)]\n",
    "        continue\n",
    "    \n",
    "data = np.array(table)\n",
    "\n",
    "del table\n",
    "\n",
    "results = []\n",
    "for r in range(0, len(data)):\n",
    "    results.append(1)\n",
    "    \n",
    "table = []\n",
    "for r in n_combos:\n",
    "    if n_combos.index(r) % 1000 == 0:\n",
    "        print(n_combos.index(r))\n",
    "    try:\n",
    "        index1 = index_dict[r[0]]\n",
    "        row1 = descriptors.loc[index1].drop('IDs').to_list()\n",
    "        \n",
    "        tarindex1 = tar_dict[r[0]]\n",
    "        tarrow1 = targets.loc[tarindex1].drop('IDs').to_list()\n",
    "        \n",
    "        index2 = index_dict[r[1]]\n",
    "        row2 = descriptors.loc[index2].drop('IDs').to_list()\n",
    "        \n",
    "        tarindex2 = tar_dict[r[1]]\n",
    "        tarrow2 = targets.loc[tarindex1].drop('IDs').to_list()\n",
    "        \n",
    "        row = row1 + tarrow1 + row2 + tarrow2\n",
    "        # row = row1 + row2\n",
    "        # row = tarrow1 + tarrow2\n",
    "        table.append(row)\n",
    "    except KeyboardInterrupt:\n",
    "        raise\n",
    "    except:\n",
    "        del n_combos[n_combos.index(r)]\n",
    "        continue\n",
    "    \n",
    "data1 = np.array(table)\n",
    "\n",
    "del table\n",
    "\n",
    "results1 = []\n",
    "for r in range(0, len(data1)):\n",
    "    results1.append(0)\n",
    "    \n",
    "file = np.concatenate((data, data1), axis=0)\n",
    "results = results + results1\n",
    "del data, data1\n",
    "print('dataset shape:')\n",
    "print(file.shape)\n",
    "\n",
    "val_set = pd.DataFrame(file, columns=col_names)\n",
    "val_results = results\n",
    "val_combos = existing_combinations + n_combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ecb1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "drugspace = [x for x in all_drugs if x not in val_drugspace]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f7f07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_dict = {}\n",
    "for i in drugspace:\n",
    "    combo_dict[i] = []\n",
    "for i in combos:\n",
    "    try:\n",
    "        x = combo_dict[i[0]]\n",
    "        x = combo_dict[i[1]]\n",
    "        combo_dict[i[0]].append(i[1])\n",
    "        combo_dict[i[1]].append(i[0])\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc7a5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_combinations = []\n",
    "for i in drugspace:\n",
    "    ingredients = combo_dict[i]\n",
    "    for i1 in ingredients:\n",
    "        existing_combinations.append([i, i1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0353ac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_combinations = list(map(list, set(map(frozenset, existing_combinations))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca06f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_combos = []\n",
    "count = len(existing_combinations)\n",
    "while len(n_combos) < count:\n",
    "    ingredient1 = 0\n",
    "    ingredient2 = 0\n",
    "    while ingredient1 == ingredient2:\n",
    "        ingredient1 = random.choice(drugspace)\n",
    "        ingredient2 = random.choice(drugspace)\n",
    "    if ingredient2 in combo_dict[ingredient1] or ingredient1 in combo_dict[ingredient2]:\n",
    "        continue\n",
    "    if [ingredient1, ingredient2] not in n_combos and [ingredient2, ingredient1] not in n_combos:\n",
    "        n_combos.append([ingredient1, ingredient2])\n",
    "        if len(n_combos) % 1000 == 0:\n",
    "            print(len(n_combos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3f0a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct dataset\n",
    "\n",
    "table = []\n",
    "for r in existing_combinations:\n",
    "    if existing_combinations.index(r) % 1000 == 0:\n",
    "        print(existing_combinations.index(r))\n",
    "    try:\n",
    "        index1 = index_dict[r[0]]\n",
    "        row1 = descriptors.loc[index1].drop('IDs').to_list()\n",
    "        \n",
    "        tarindex1 = tar_dict[r[0]]\n",
    "        tarrow1 = targets.loc[tarindex1].drop('IDs').to_list()\n",
    "        \n",
    "        index2 = index_dict[r[1]]\n",
    "        row2 = descriptors.loc[index2].drop('IDs').to_list()\n",
    "        \n",
    "        tarindex2 = tar_dict[r[1]]\n",
    "        tarrow2 = targets.loc[tarindex1].drop('IDs').to_list()\n",
    "        \n",
    "        row = row1 + tarrow1 + row2 + tarrow2\n",
    "        # row = row1 + row2\n",
    "        # row = tarrow1 + tarrow2\n",
    "        table.append(row)\n",
    "    except KeyboardInterrupt:\n",
    "        raise\n",
    "    except:\n",
    "        # del existing_combinations[existing_combinations.index(r)]\n",
    "        continue\n",
    "\n",
    "data = np.array(table)\n",
    "\n",
    "del table\n",
    "\n",
    "results = []\n",
    "for r in range(0, len(data)):\n",
    "    results.append(1)\n",
    "\n",
    "table = []\n",
    "for r in n_combos:\n",
    "    if n_combos.index(r) % 1000 == 0:\n",
    "        print(n_combos.index(r))\n",
    "    try:\n",
    "        index1 = index_dict[r[0]]\n",
    "        row1 = descriptors.loc[index1].drop('IDs').to_list()\n",
    "        \n",
    "        tarindex1 = tar_dict[r[0]]\n",
    "        tarrow1 = targets.loc[tarindex1].drop('IDs').to_list()\n",
    "        \n",
    "        index2 = index_dict[r[1]]\n",
    "        row2 = descriptors.loc[index2].drop('IDs').to_list()\n",
    "        \n",
    "        tarindex2 = tar_dict[r[1]]\n",
    "        tarrow2 = targets.loc[tarindex1].drop('IDs').to_list()\n",
    "        \n",
    "        row = row1 + tarrow1 + row2 + tarrow2\n",
    "        # row = row1 + row2\n",
    "        # row = tarrow1 + tarrow2\n",
    "        table.append(row)\n",
    "    except KeyboardInterrupt:\n",
    "        raise\n",
    "    except:\n",
    "        # del n_combos[n_combos.index(r)]\n",
    "        continue\n",
    "\n",
    "data1 = np.array(table)\n",
    "\n",
    "del table\n",
    "\n",
    "results1 = []\n",
    "for r in range(0, len(data1)):\n",
    "    results1.append(0)\n",
    "\n",
    "file = np.concatenate((data, data1), axis=0)\n",
    "results = results + results1\n",
    "del data, data1\n",
    "file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1b64a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = pd.DataFrame(file, columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd643cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = descriptors.columns.to_list()[1:] + targets.columns.to_list()[1:]\n",
    "\n",
    "drop = [x for x in file.columns if x in names]\n",
    "temp1 = file.drop(columns=drop)\n",
    "temp1.columns = drop\n",
    "drop = [x for x in file.columns if x not in names]\n",
    "temp2 = file.drop(columns=drop)\n",
    "temp2.columns = drop\n",
    "\n",
    "file = pd.concat([file, pd.concat([temp1, temp2], axis=1)]).reset_index(drop=True)\n",
    "del temp1, temp2\n",
    "results = results + results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669b1907",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47763c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "file, results = shuffle(file, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8054ad82",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = np.array(file)\n",
    "results = np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be01ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, precision_score, recall_score, roc_curve, auc, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6cdd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 fold cross validation\n",
    "# temp scaling code obtained here: https://sourajit16-02-93.medium.com/neural-network-calibration-46997f8c872c\n",
    "\n",
    "acc = []\n",
    "roc = []\n",
    "prc = []\n",
    "prec = []\n",
    "f1 = []\n",
    "rec = []\n",
    "spec = []\n",
    "curve = []\n",
    "prc_curve = []\n",
    "models = {}\n",
    "scalers = {}\n",
    "test_sets = {}\n",
    "test_results = {}\n",
    "temperature = []\n",
    "iteration = 0\n",
    "\n",
    "for train, val in kf.split(file, results):\n",
    "    iteration += 1\n",
    "    x_train = file[train]\n",
    "    y_train = to_categorical(results[train])\n",
    "    x_val = file[val]\n",
    "    y_val = results[val]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    x_train = scaler.fit_transform(x_train).astype('float32')\n",
    "    x_val = scaler.transform(x_val).astype('float32')\n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    n_cols = x_train.shape[1]\n",
    "    model.add(tf.keras.layers.Dense(n_cols, activation='relu', input_shape=(n_cols,)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(tf.keras.layers.Dense(int(n_cols / 2), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(2, activation='linear'))\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.0005, momentum=0.5), \n",
    "          loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "         metrics=[tf.keras.metrics.AUC(curve='roc'), 'accuracy', tfa.metrics.F1Score(num_classes=2)])\n",
    "    \n",
    "    early_stopping_monitor = EarlyStopping(patience=3)\n",
    "    model.fit(x_train, y_train, validation_data=(x_val, to_categorical(y_val)), epochs=100, batch_size = 32, callbacks=[early_stopping_monitor])\n",
    "    \n",
    "    temp = tf.Variable(initial_value=1.0, trainable=True)\n",
    "    y_pred = model.predict(x_val)\n",
    "    y_test = to_categorical(y_val)\n",
    "\n",
    "    def compute_loss():\n",
    "        y_pred_model_w_temp = tf.math.divide(y_pred, temp)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(tf.convert_to_tensor(y_test), y_pred_model_w_temp))\n",
    "        return loss\n",
    "\n",
    "    optimizer = tf.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "    print('Temperature Initial value: {}'.format(temp.numpy()))\n",
    "\n",
    "    for i in range(300):\n",
    "         opts = optimizer.minimize(compute_loss, var_list=[temp])\n",
    "\n",
    "    print('Temperature Final value: {}'.format(temp.numpy()))\n",
    "    \n",
    "    temperature.append(temp.numpy())\n",
    "    \n",
    "    predictions = model.predict(x_val)\n",
    "    predictions = tf.nn.softmax(predictions / temp)\n",
    "    \n",
    "    binary_pred = []\n",
    "    for i in predictions:\n",
    "        if i[0] > i[1]:\n",
    "            binary_pred.append(0)\n",
    "        else:\n",
    "            binary_pred.append(1)\n",
    "            \n",
    "    acc.append(accuracy_score(y_val, binary_pred))\n",
    "    roc.append(roc_auc_score(y_val, binary_pred))\n",
    "    prc.append(average_precision_score(y_val, binary_pred))\n",
    "    f1.append(f1_score(y_val, binary_pred))\n",
    "    rec.append(recall_score(y_val, binary_pred))\n",
    "    prec.append(precision_score(y_val, binary_pred))\n",
    "    spec.append(recall_score(y_val, binary_pred, pos_label=0))\n",
    "    \n",
    "    print(roc_auc_score(y_val, binary_pred))\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_val, predictions[:,1])\n",
    "    curve.append([fpr, tpr, thresholds])\n",
    "    precision, recall, thresholds = precision_recall_curve(y_val, predictions[:,1])\n",
    "    prc_curve.append([precision, recall, thresholds])\n",
    "    \n",
    "    models[f'model{iteration}'] = model\n",
    "    scalers[f'scaler{iteration}'] = scaler\n",
    "    test_sets[f'set{iteration}'] = x_val\n",
    "    test_results[f'set{iteration}'] = y_val\n",
    "    \n",
    "    del x_train, x_val\n",
    "    \n",
    "    del model\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6ec99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(all_drugs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20312f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrapping ensemble testing of one model, two model, four model, and combined\n",
    "\n",
    "subsets = list(chunks(all_drugs, 500))\n",
    "acc = []\n",
    "roc = []\n",
    "prc = []\n",
    "prec = []\n",
    "f1 = []\n",
    "rec = []\n",
    "spec = []\n",
    "curve = []\n",
    "prc_curve = []\n",
    "\n",
    "acc1 = []\n",
    "roc1 = []\n",
    "prc1 = []\n",
    "prec1 = []\n",
    "f11 = []\n",
    "rec1 = []\n",
    "spec1 = []\n",
    "curve1 = []\n",
    "prc_curve1 = []\n",
    "\n",
    "acc2 = []\n",
    "roc2 = []\n",
    "prc2 = []\n",
    "prec2 = []\n",
    "f12 = []\n",
    "rec2 = []\n",
    "spec2 = []\n",
    "curve2 = []\n",
    "prc_curve2 = []\n",
    "\n",
    "acc4 = []\n",
    "roc4 = []\n",
    "prc4 = []\n",
    "prec4 = []\n",
    "f14 = []\n",
    "rec4 = []\n",
    "spec4 = []\n",
    "curve4 = []\n",
    "prc_curve4 = []\n",
    "\n",
    "for i in subsets:\n",
    "    val_drugspace = i\n",
    "    val_set, val_results, val_combos = create_validation_set(val_drugspace)\n",
    "    drugspace = [x for x in all_drugs if x not in val_drugspace]\n",
    "    model1s = []\n",
    "    model2s = []\n",
    "    model4s = []\n",
    "    \n",
    "    combo_dict = {}\n",
    "    for i in drugspace:\n",
    "        combo_dict[i] = []\n",
    "    for i in combos:\n",
    "        try:\n",
    "            x = combo_dict[i[0]]\n",
    "            x = combo_dict[i[1]]\n",
    "            combo_dict[i[0]].append(i[1])\n",
    "            combo_dict[i[1]].append(i[0])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    existing_combinations = []\n",
    "    for i in drugspace:\n",
    "        ingredients = combo_dict[i]\n",
    "        for i1 in ingredients:\n",
    "            existing_combinations.append([i, i1])\n",
    "\n",
    "    existing_combinations = list(map(list, set(map(frozenset, existing_combinations))))\n",
    "    \n",
    "    for r in range(0, 3):\n",
    "        if r == 0:\n",
    "            for i in range(0, len(existing_combinations)):\n",
    "                existing_combinations.append([existing_combinations[i][1], existing_combinations[i][0]])\n",
    "                \n",
    "            n_combos = []\n",
    "            count = len(existing_combinations) / 2\n",
    "            while len(n_combos) < count:\n",
    "                ingredient1 = 0\n",
    "                ingredient2 = 0\n",
    "                while ingredient1 == ingredient2:\n",
    "                    ingredient1 = random.choice(drugspace)\n",
    "                    ingredient2 = random.choice(drugspace)\n",
    "                if ingredient2 in combo_dict[ingredient1] or ingredient1 in combo_dict[ingredient2]:\n",
    "                    continue\n",
    "                if [ingredient1, ingredient2] not in n_combos and [ingredient2, ingredient1] not in n_combos:\n",
    "                    n_combos.append([ingredient1, ingredient2])\n",
    "                    if len(n_combos) % 1000 == 0:\n",
    "                        print(len(n_combos))\n",
    "\n",
    "            for i in range(0, len(n_combos)):\n",
    "                n_combos.append([n_combos[i][1], n_combos[i][0]])\n",
    "                \n",
    "            table = []\n",
    "            for r in existing_combinations:\n",
    "                if existing_combinations.index(r) % 1000 == 0:\n",
    "                    print(existing_combinations.index(r))\n",
    "                try:\n",
    "                    index1 = index_dict[r[0]]\n",
    "                    row1 = descriptors.loc[index1].drop('IDs').to_list()\n",
    "\n",
    "                    tarindex1 = tar_dict[r[0]]\n",
    "                    tarrow1 = targets.loc[tarindex1].drop('IDs').to_list()\n",
    "\n",
    "                    index2 = index_dict[r[1]]\n",
    "                    row2 = descriptors.loc[index2].drop('IDs').to_list()\n",
    "\n",
    "                    tarindex2 = tar_dict[r[1]]\n",
    "                    tarrow2 = targets.loc[tarindex1].drop('IDs').to_list()\n",
    "\n",
    "                    row = row1 + tarrow1 + row2 + tarrow2\n",
    "                    # row = row1 + row2\n",
    "                    # row = tarrow1 + tarrow2\n",
    "                    table.append(row)\n",
    "                except KeyboardInterrupt:\n",
    "                    raise\n",
    "                except:\n",
    "                    del existing_combinations[existing_combinations.index(r)]\n",
    "                    continue\n",
    "\n",
    "            data = np.array(table)\n",
    "\n",
    "            del table\n",
    "\n",
    "            results = []\n",
    "            for r in range(0, len(data)):\n",
    "                results.append(1)\n",
    "\n",
    "            table = []\n",
    "            for r in n_combos:\n",
    "                if n_combos.index(r) % 1000 == 0:\n",
    "                    print(n_combos.index(r))\n",
    "                try:\n",
    "                    index1 = index_dict[r[0]]\n",
    "                    row1 = descriptors.loc[index1].drop('IDs').to_list()\n",
    "\n",
    "                    tarindex1 = tar_dict[r[0]]\n",
    "                    tarrow1 = targets.loc[tarindex1].drop('IDs').to_list()\n",
    "\n",
    "                    index2 = index_dict[r[1]]\n",
    "                    row2 = descriptors.loc[index2].drop('IDs').to_list()\n",
    "\n",
    "                    tarindex2 = tar_dict[r[1]]\n",
    "                    tarrow2 = targets.loc[tarindex1].drop('IDs').to_list()\n",
    "\n",
    "                    row = row1 + tarrow1 + row2 + tarrow2\n",
    "                    # row = row1 + row2\n",
    "                    # row = tarrow1 + tarrow2\n",
    "                    table.append(row)\n",
    "                except KeyboardInterrupt:\n",
    "                    raise\n",
    "                except:\n",
    "                    del n_combos[n_combos.index(r)]\n",
    "                    continue\n",
    "\n",
    "            data1 = np.array(table)\n",
    "\n",
    "            del table\n",
    "\n",
    "            results1 = []\n",
    "            for r in range(0, len(data1)):\n",
    "                results1.append(0)\n",
    "\n",
    "            file = np.concatenate((data, data1), axis=0)\n",
    "            results = results + results1\n",
    "            del data, data1\n",
    "            file.shape\n",
    "            \n",
    "            file = pd.DataFrame(file, columns=col_names)\n",
    "            \n",
    "            scale_names = descriptors.columns.to_list()[1:]\n",
    "            scale_names = [x for x in scale_names if x in file.columns]\n",
    "            temp = [x + ' - 1' for x in scale_names]\n",
    "            scale_names = scale_names + temp\n",
    "\n",
    "            x_train, x_val, y_train, y_val = train_test_split(file, results, test_size=0.2)\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "\n",
    "            standard_transformer = Pipeline(steps=[\n",
    "                    ('standard', scaler)])\n",
    "\n",
    "            preprocessor = ColumnTransformer(\n",
    "                        remainder='passthrough', transformers=[\n",
    "                            ('std', standard_transformer , scale_names),\n",
    "                        ])\n",
    "            x_train = preprocessor.fit_transform(x_train)\n",
    "            x_val = preprocessor.transform(x_val)\n",
    "\n",
    "            x_train = x_train.astype('float32')\n",
    "            y_train = to_categorical(y_train)\n",
    "\n",
    "            print('training model')\n",
    "            model = tf.keras.models.Sequential()\n",
    "            n_cols = x_train.shape[1]\n",
    "            model.add(tf.keras.layers.Dense(n_cols, activation='relu', input_shape=(n_cols,)))\n",
    "            model.add(Dropout(0.2))\n",
    "            model.add(tf.keras.layers.Dense(int(n_cols / 2), activation='relu'))\n",
    "            model.add(Dropout(0.5))\n",
    "            model.add(tf.keras.layers.Dense(2, activation='linear'))\n",
    "\n",
    "            model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.0005, momentum=0.5), \n",
    "                      loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                     metrics=[tf.keras.metrics.AUC(), 'accuracy', tf.keras.metrics.Precision()])\n",
    "\n",
    "            early_stopping_monitor = EarlyStopping(patience=3)\n",
    "            model.fit(x_train, y_train, validation_data=(x_val, to_categorical(y_val)), epochs=100, batch_size = 32, callbacks=[early_stopping_monitor])\n",
    "\n",
    "            temp = tf.Variable(initial_value=1.0, trainable=True)\n",
    "            y_pred = model.predict(x_val)\n",
    "            y_test = to_categorical(y_val)\n",
    "\n",
    "            def compute_loss():\n",
    "                y_pred_model_w_temp = tf.math.divide(y_pred, temp)\n",
    "                loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(tf.convert_to_tensor(y_test), y_pred_model_w_temp))\n",
    "                return loss\n",
    "\n",
    "            optimizer = tf.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "            print('Temperature Initial value: {}'.format(temp.numpy()))\n",
    "\n",
    "            for i in range(300):\n",
    "                 opts = optimizer.minimize(compute_loss, var_list=[temp])\n",
    "\n",
    "            print('Temperature Final value: {}'.format(temp.numpy()))\n",
    "            \n",
    "            model1s.append([model, preprocessor, temp.numpy()])\n",
    "        else:\n",
    "            existing_combinations = []\n",
    "            for i in drugspace:\n",
    "                ingredients = combo_dict[i]\n",
    "                for i1 in ingredients:\n",
    "                    existing_combinations.append([i, i1])\n",
    "\n",
    "            existing_combinations = list(map(list, set(map(frozenset, existing_combinations))))\n",
    "            \n",
    "            if r == 1:\n",
    "                c = 2\n",
    "            else:\n",
    "                c = 4\n",
    "                \n",
    "            import math\n",
    "            count = math.ceil(len(existing_combinations) / c)\n",
    "            subset = list(chunks(existing_combinations, int(count)))\n",
    "            \n",
    "            for i in subset:\n",
    "                existing_combinations = i\n",
    "\n",
    "                for i in range(0, len(existing_combinations)):\n",
    "                    existing_combinations.append([existing_combinations[i][1], existing_combinations[i][0]])\n",
    "\n",
    "                n_combos = []\n",
    "                count = len(existing_combinations) / 2\n",
    "                while len(n_combos) < count:\n",
    "                    ingredient1 = 0\n",
    "                    ingredient2 = 0\n",
    "                    while ingredient1 == ingredient2:\n",
    "                        ingredient1 = random.choice(drugspace)\n",
    "                        ingredient2 = random.choice(drugspace)\n",
    "                    if ingredient2 in combo_dict[ingredient1] or ingredient1 in combo_dict[ingredient2]:\n",
    "                        continue\n",
    "                    if [ingredient1, ingredient2] not in n_combos and [ingredient2, ingredient1] not in n_combos:\n",
    "                        n_combos.append([ingredient1, ingredient2])\n",
    "                        if len(n_combos) % 1000 == 0:\n",
    "                            print(len(n_combos))\n",
    "\n",
    "                for i in range(0, len(n_combos)):\n",
    "                    n_combos.append([n_combos[i][1], n_combos[i][0]])\n",
    "\n",
    "                print('constructing dataset')\n",
    "                table = []\n",
    "                for r in existing_combinations:\n",
    "                    if existing_combinations.index(r) % 1000 == 0:\n",
    "                        print(existing_combinations.index(r))\n",
    "                    try:\n",
    "                        index1 = index_dict[r[0]]\n",
    "                        row1 = descriptors.loc[index1].drop('IDs').to_list()\n",
    "\n",
    "                        tarindex1 = tar_dict[r[0]]\n",
    "                        tarrow1 = targets.loc[tarindex1].drop('IDs').to_list()\n",
    "\n",
    "                        index2 = index_dict[r[1]]\n",
    "                        row2 = descriptors.loc[index2].drop('IDs').to_list()\n",
    "\n",
    "                        tarindex2 = tar_dict[r[1]]\n",
    "                        tarrow2 = targets.loc[tarindex1].drop('IDs').to_list()\n",
    "\n",
    "                        row = row1 + tarrow1 + row2 + tarrow2\n",
    "                        # row = row1 + row2\n",
    "                        # row = tarrow1 + tarrow2\n",
    "                        table.append(row)\n",
    "                    except KeyboardInterrupt:\n",
    "                        raise\n",
    "                    except:\n",
    "                        del existing_combinations[existing_combinations.index(r)]\n",
    "                        continue\n",
    "\n",
    "                data = np.array(table)\n",
    "\n",
    "                del table\n",
    "\n",
    "                results = []\n",
    "                for r in range(0, len(data)):\n",
    "                    results.append(1)\n",
    "\n",
    "                table = []\n",
    "                for r in n_combos:\n",
    "                    if n_combos.index(r) % 1000 == 0:\n",
    "                        print(n_combos.index(r))\n",
    "                    try:\n",
    "                        index1 = index_dict[r[0]]\n",
    "                        row1 = descriptors.loc[index1].drop('IDs').to_list()\n",
    "\n",
    "                        tarindex1 = tar_dict[r[0]]\n",
    "                        tarrow1 = targets.loc[tarindex1].drop('IDs').to_list()\n",
    "\n",
    "                        index2 = index_dict[r[1]]\n",
    "                        row2 = descriptors.loc[index2].drop('IDs').to_list()\n",
    "\n",
    "                        tarindex2 = tar_dict[r[1]]\n",
    "                        tarrow2 = targets.loc[tarindex1].drop('IDs').to_list()\n",
    "\n",
    "                        row = row1 + tarrow1 + row2 + tarrow2\n",
    "                        # row = row1 + row2\n",
    "                        # row = tarrow1 + tarrow2\n",
    "                        table.append(row)\n",
    "                    except KeyboardInterrupt:\n",
    "                        raise\n",
    "                    except:\n",
    "                        del n_combos[n_combos.index(r)]\n",
    "                        continue\n",
    "\n",
    "                data1 = np.array(table)\n",
    "\n",
    "                del table\n",
    "\n",
    "                results1 = []\n",
    "                for r in range(0, len(data1)):\n",
    "                    results1.append(0)\n",
    "\n",
    "                file = np.concatenate((data, data1), axis=0)\n",
    "                results = results + results1\n",
    "                del data, data1\n",
    "                print('dataset shape')\n",
    "                print(file.shape)\n",
    "\n",
    "                file = pd.DataFrame(file, columns=col_names)\n",
    "\n",
    "                scale_names = descriptors.columns.to_list()[1:]\n",
    "                scale_names = [x for x in scale_names if x in file.columns]\n",
    "                temp = [x + ' - 1' for x in scale_names]\n",
    "                scale_names = scale_names + temp\n",
    "\n",
    "                x_train, x_val, y_train, y_val = train_test_split(file, results, test_size=0.2)\n",
    "\n",
    "                scaler = StandardScaler()\n",
    "\n",
    "                standard_transformer = Pipeline(steps=[\n",
    "                        ('standard', scaler)])\n",
    "\n",
    "                preprocessor = ColumnTransformer(\n",
    "                        remainder='passthrough', transformers=[\n",
    "                            ('std', standard_transformer , scale_names),\n",
    "                        ])\n",
    "                x_train = preprocessor.fit_transform(x_train)\n",
    "                x_val = preprocessor.transform(x_val)\n",
    "            \n",
    "                x_train = x_train.astype('float32')\n",
    "                y_train = to_categorical(y_train)\n",
    "\n",
    "                print('training model')\n",
    "                model = tf.keras.models.Sequential()\n",
    "                n_cols = x_train.shape[1]\n",
    "                model.add(tf.keras.layers.Dense(n_cols, activation='relu', input_shape=(n_cols,)))\n",
    "                model.add(Dropout(0.2))\n",
    "                model.add(tf.keras.layers.Dense(int(n_cols / 2), activation='relu'))\n",
    "                model.add(Dropout(0.5))\n",
    "                model.add(tf.keras.layers.Dense(2, activation='linear'))\n",
    "\n",
    "                model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.0005, momentum=0.5), \n",
    "                          loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                         metrics=[tf.keras.metrics.AUC(), 'accuracy', tf.keras.metrics.Precision()])\n",
    "\n",
    "                early_stopping_monitor = EarlyStopping(patience=3)\n",
    "                model.fit(x_train, y_train, validation_data=(x_val, to_categorical(y_val)), epochs=100, batch_size = 32, callbacks=[early_stopping_monitor])\n",
    "                \n",
    "                temp = tf.Variable(initial_value=1.0, trainable=True)\n",
    "                y_pred = model.predict(x_val)\n",
    "                y_test = to_categorical(y_val)\n",
    "\n",
    "                def compute_loss():\n",
    "                    y_pred_model_w_temp = tf.math.divide(y_pred, temp)\n",
    "                    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(tf.convert_to_tensor(y_test), y_pred_model_w_temp))\n",
    "                    return loss\n",
    "\n",
    "                optimizer = tf.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "                print('Temperature Initial value: {}'.format(temp.numpy()))\n",
    "\n",
    "                for i in range(300):\n",
    "                     opts = optimizer.minimize(compute_loss, var_list=[temp])\n",
    "\n",
    "                print('Temperature Final value: {}'.format(temp.numpy()))\n",
    "                \n",
    "                if c == 2:\n",
    "                    model2s.append([model, preprocessor, temp.numpy()])\n",
    "                else:\n",
    "                    model4s.append([model, preprocessor, temp.numpy()])\n",
    "                    \n",
    "    predictions = []\n",
    "    for r in range(0, 3):\n",
    "        if r == 0:\n",
    "            pred = model1s[0][0].predict(model1s[0][1].transform(val_set))\n",
    "            pred = tf.nn.softmax(pred / model1s[0][2])\n",
    "            pred1 = pred\n",
    "            predictions.append(pred)\n",
    "        elif r == 1:\n",
    "            temp_pred = []\n",
    "            for r1 in range(0, 2):\n",
    "                pred = model2s[r1][0].predict(model2s[r1][1].transform(val_set))\n",
    "                pred = tf.nn.softmax(pred / model2s[r1][2])\n",
    "                temp_pred.append(pred)\n",
    "            predictions.append(np.mean(np.array(temp_pred), axis=0))\n",
    "            pred2 = np.mean(np.array(temp_pred), axis=0)\n",
    "        else:\n",
    "            temp_pred = []\n",
    "            for r1 in range(0, 4):\n",
    "                pred = model4s[r1][0].predict(model4s[r1][1].transform(val_set))\n",
    "                pred = tf.nn.softmax(pred / model4s[r1][2])\n",
    "                temp_pred.append(pred)\n",
    "            predictions.append(np.mean(np.array(temp_pred), axis=0))\n",
    "            pred4 = np.mean(np.array(temp_pred), axis=0)\n",
    "\n",
    "    predictions = np.mean(np.array(predictions), axis=0)\n",
    "\n",
    "    y_val = val_results\n",
    "\n",
    "    binary_pred = []\n",
    "    for i in predictions:\n",
    "        if i[0] > i[1]:\n",
    "            binary_pred.append(0)\n",
    "        else:\n",
    "            binary_pred.append(1)\n",
    "\n",
    "    acc.append(accuracy_score(y_val, binary_pred))\n",
    "    f1.append(f1_score(y_val, binary_pred))\n",
    "    rec.append(recall_score(y_val, binary_pred))\n",
    "    prec.append(precision_score(y_val, binary_pred))\n",
    "    spec.append(recall_score(y_val, binary_pred, pos_label=0))\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_val, predictions[:,1])\n",
    "    curve.append([fpr, tpr, thresholds])\n",
    "    roc.append(auc(fpr, tpr))\n",
    "    precision, recall, thresholds = precision_recall_curve(y_val, predictions[:,1])\n",
    "    prc_curve.append([precision, recall, thresholds])\n",
    "    prc.append(auc(recall, precision))\n",
    "\n",
    "    predictions = pred1\n",
    "\n",
    "    y_val = val_results\n",
    "\n",
    "    binary_pred = []\n",
    "    for i in predictions:\n",
    "        if i[0] > i[1]:\n",
    "            binary_pred.append(0)\n",
    "        else:\n",
    "            binary_pred.append(1)\n",
    "\n",
    "    acc1.append(accuracy_score(y_val, binary_pred))\n",
    "    f11.append(f1_score(y_val, binary_pred))\n",
    "    rec1.append(recall_score(y_val, binary_pred))\n",
    "    prec1.append(precision_score(y_val, binary_pred))\n",
    "    spec1.append(recall_score(y_val, binary_pred, pos_label=0))\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_val, predictions[:,1])\n",
    "    curve1.append([fpr, tpr, thresholds])\n",
    "    roc1.append(auc(fpr, tpr))\n",
    "    precision, recall, thresholds = precision_recall_curve(y_val, predictions[:,1])\n",
    "    prc_curve1.append([precision, recall, thresholds])\n",
    "    prc1.append(auc(recall, precision))\n",
    "\n",
    "    predictions = pred2\n",
    "\n",
    "    y_val = val_results\n",
    "\n",
    "    binary_pred = []\n",
    "    for i in predictions:\n",
    "        if i[0] > i[1]:\n",
    "            binary_pred.append(0)\n",
    "        else:\n",
    "            binary_pred.append(1)\n",
    "\n",
    "    acc2.append(accuracy_score(y_val, binary_pred))\n",
    "    f12.append(f1_score(y_val, binary_pred))\n",
    "    rec2.append(recall_score(y_val, binary_pred))\n",
    "    prec2.append(precision_score(y_val, binary_pred))\n",
    "    spec2.append(recall_score(y_val, binary_pred, pos_label=0))\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_val, predictions[:,1])\n",
    "    curve2.append([fpr, tpr, thresholds])\n",
    "    roc2.append(auc(fpr, tpr))\n",
    "    precision, recall, thresholds = precision_recall_curve(y_val, predictions[:,1])\n",
    "    prc_curve2.append([precision, recall, thresholds])\n",
    "    prc2.append(auc(recall, precision))\n",
    "\n",
    "    predictions = pred4\n",
    "\n",
    "    y_val = val_results\n",
    "\n",
    "    binary_pred = []\n",
    "    for i in predictions:\n",
    "        if i[0] > i[1]:\n",
    "            binary_pred.append(0)\n",
    "        else:\n",
    "            binary_pred.append(1)\n",
    "\n",
    "    acc4.append(accuracy_score(y_val, binary_pred))\n",
    "    f14.append(f1_score(y_val, binary_pred))\n",
    "    rec4.append(recall_score(y_val, binary_pred))\n",
    "    prec4.append(precision_score(y_val, binary_pred))\n",
    "    spec4.append(recall_score(y_val, binary_pred, pos_label=0))\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_val, predictions[:,1])\n",
    "    curve4.append([fpr, tpr, thresholds])\n",
    "    roc4.append(auc(fpr, tpr))\n",
    "    precision, recall, thresholds = precision_recall_curve(y_val, predictions[:,1])\n",
    "    prc_curve4.append([precision, recall, thresholds])\n",
    "    prc4.append(auc(recall, precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d17f46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train final models\n",
    "\n",
    "drugspace = all_drugs\n",
    "model1s = []\n",
    "model2s = []\n",
    "model4s = []\n",
    "\n",
    "combo_dict = {}\n",
    "for i in drugspace:\n",
    "    combo_dict[i] = []\n",
    "for i in combos:\n",
    "    try:\n",
    "        x = combo_dict[i[0]]\n",
    "        x = combo_dict[i[1]]\n",
    "        combo_dict[i[0]].append(i[1])\n",
    "        combo_dict[i[1]].append(i[0])\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "existing_combinations = []\n",
    "for i in drugspace:\n",
    "    ingredients = combo_dict[i]\n",
    "    for i1 in ingredients:\n",
    "        existing_combinations.append([i, i1])\n",
    "\n",
    "existing_combinations = list(map(list, set(map(frozenset, existing_combinations))))\n",
    "\n",
    "for r in range(0, 3):\n",
    "    if r == 0:\n",
    "        for i in range(0, len(existing_combinations)):\n",
    "            existing_combinations.append([existing_combinations[i][1], existing_combinations[i][0]])\n",
    "\n",
    "        n_combos = []\n",
    "        count = len(existing_combinations) / 2\n",
    "        while len(n_combos) < count:\n",
    "            ingredient1 = 0\n",
    "            ingredient2 = 0\n",
    "            while ingredient1 == ingredient2:\n",
    "                ingredient1 = random.choice(drugspace)\n",
    "                ingredient2 = random.choice(drugspace)\n",
    "            if ingredient2 in combo_dict[ingredient1] or ingredient1 in combo_dict[ingredient2]:\n",
    "                continue\n",
    "            if [ingredient1, ingredient2] not in n_combos and [ingredient2, ingredient1] not in n_combos:\n",
    "                n_combos.append([ingredient1, ingredient2])\n",
    "                if len(n_combos) % 1000 == 0:\n",
    "                    print(len(n_combos))\n",
    "\n",
    "        for i in range(0, len(n_combos)):\n",
    "            n_combos.append([n_combos[i][1], n_combos[i][0]])\n",
    "\n",
    "        table = []\n",
    "        for r in existing_combinations:\n",
    "            if existing_combinations.index(r) % 1000 == 0:\n",
    "                print(existing_combinations.index(r))\n",
    "            try:\n",
    "                index1 = index_dict[r[0]]\n",
    "                row1 = descriptors.loc[index1].drop('IDs').to_list()\n",
    "\n",
    "                tarindex1 = tar_dict[r[0]]\n",
    "                tarrow1 = targets.loc[tarindex1].drop('IDs').to_list()\n",
    "\n",
    "                index2 = index_dict[r[1]]\n",
    "                row2 = descriptors.loc[index2].drop('IDs').to_list()\n",
    "\n",
    "                tarindex2 = tar_dict[r[1]]\n",
    "                tarrow2 = targets.loc[tarindex1].drop('IDs').to_list()\n",
    "\n",
    "                row = row1 + tarrow1 + row2 + tarrow2\n",
    "                # row = row1 + row2\n",
    "                # row = tarrow1 + tarrow2\n",
    "                table.append(row)\n",
    "            except KeyboardInterrupt:\n",
    "                raise\n",
    "            except:\n",
    "                del existing_combinations[existing_combinations.index(r)]\n",
    "                continue\n",
    "\n",
    "        data = np.array(table)\n",
    "\n",
    "        del table\n",
    "\n",
    "        results = []\n",
    "        for r in range(0, len(data)):\n",
    "            results.append(1)\n",
    "\n",
    "        table = []\n",
    "        for r in n_combos:\n",
    "            if n_combos.index(r) % 1000 == 0:\n",
    "                print(n_combos.index(r))\n",
    "            try:\n",
    "                index1 = index_dict[r[0]]\n",
    "                row1 = descriptors.loc[index1].drop('IDs').to_list()\n",
    "\n",
    "                tarindex1 = tar_dict[r[0]]\n",
    "                tarrow1 = targets.loc[tarindex1].drop('IDs').to_list()\n",
    "\n",
    "                index2 = index_dict[r[1]]\n",
    "                row2 = descriptors.loc[index2].drop('IDs').to_list()\n",
    "\n",
    "                tarindex2 = tar_dict[r[1]]\n",
    "                tarrow2 = targets.loc[tarindex1].drop('IDs').to_list()\n",
    "\n",
    "                row = row1 + tarrow1 + row2 + tarrow2\n",
    "                # row = row1 + row2\n",
    "                # row = tarrow1 + tarrow2\n",
    "                table.append(row)\n",
    "            except KeyboardInterrupt:\n",
    "                raise\n",
    "            except:\n",
    "                del n_combos[n_combos.index(r)]\n",
    "                continue\n",
    "\n",
    "        data1 = np.array(table)\n",
    "\n",
    "        del table\n",
    "\n",
    "        results1 = []\n",
    "        for r in range(0, len(data1)):\n",
    "            results1.append(0)\n",
    "\n",
    "        file = np.concatenate((data, data1), axis=0)\n",
    "        results = results + results1\n",
    "        del data, data1\n",
    "        file.shape\n",
    "\n",
    "        file = pd.DataFrame(file, columns=col_names)\n",
    "\n",
    "        scale_names = descriptors.columns.to_list()[1:]\n",
    "        scale_names = [x for x in scale_names if x in file.columns]\n",
    "        temp = [x + ' - 1' for x in scale_names]\n",
    "        scale_names = scale_names + temp\n",
    "\n",
    "        x_train, x_val, y_train, y_val = train_test_split(file, results, test_size=0.2)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        standard_transformer = Pipeline(steps=[\n",
    "                ('standard', scaler)])\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "                        remainder='passthrough', transformers=[\n",
    "                            ('std', standard_transformer , scale_names),\n",
    "                        ])\n",
    "        x_train = preprocessor.fit_transform(x_train)\n",
    "        x_val = preprocessor.transform(x_val)\n",
    "\n",
    "        x_train = x_train.astype('float32')\n",
    "        y_train = to_categorical(y_train)\n",
    "\n",
    "        print('training model')\n",
    "        model = tf.keras.models.Sequential()\n",
    "        n_cols = x_train.shape[1]\n",
    "        model.add(tf.keras.layers.Dense(n_cols, activation='relu', input_shape=(n_cols,)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(tf.keras.layers.Dense(int(n_cols / 2), activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(tf.keras.layers.Dense(2, activation='linear'))\n",
    "\n",
    "        model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.0005, momentum=0.5), \n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=[tf.keras.metrics.AUC(), 'accuracy', tf.keras.metrics.Precision()])\n",
    "\n",
    "        early_stopping_monitor = EarlyStopping(patience=3)\n",
    "        model.fit(x_train, y_train, validation_data=(x_val, to_categorical(y_val)), epochs=100, batch_size = 32, callbacks=[early_stopping_monitor])\n",
    "\n",
    "        temp = tf.Variable(initial_value=1.0, trainable=True)\n",
    "        y_pred = model.predict(x_val)\n",
    "        y_test = to_categorical(y_val)\n",
    "\n",
    "        def compute_loss():\n",
    "            y_pred_model_w_temp = tf.math.divide(y_pred, temp)\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(tf.convert_to_tensor(y_test), y_pred_model_w_temp))\n",
    "            return loss\n",
    "\n",
    "        optimizer = tf.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "        print('Temperature Initial value: {}'.format(temp.numpy()))\n",
    "\n",
    "        for i in range(300):\n",
    "             opts = optimizer.minimize(compute_loss, var_list=[temp])\n",
    "\n",
    "        print('Temperature Final value: {}'.format(temp.numpy()))\n",
    "\n",
    "        model1s.append([model, preprocessor, temp.numpy()])\n",
    "    else:\n",
    "        existing_combinations = []\n",
    "        for i in drugspace:\n",
    "            ingredients = combo_dict[i]\n",
    "            for i1 in ingredients:\n",
    "                existing_combinations.append([i, i1])\n",
    "\n",
    "        existing_combinations = list(map(list, set(map(frozenset, existing_combinations))))\n",
    "\n",
    "        if r == 1:\n",
    "            c = 2\n",
    "        else:\n",
    "            c = 4\n",
    "\n",
    "        import math\n",
    "        count = math.ceil(len(existing_combinations) / c)\n",
    "        subset = list(chunks(existing_combinations, int(count)))\n",
    "\n",
    "        for i in subset:\n",
    "            existing_combinations = i\n",
    "\n",
    "            for i in range(0, len(existing_combinations)):\n",
    "                existing_combinations.append([existing_combinations[i][1], existing_combinations[i][0]])\n",
    "\n",
    "            n_combos = []\n",
    "            count = len(existing_combinations) / 2\n",
    "            while len(n_combos) < count:\n",
    "                ingredient1 = 0\n",
    "                ingredient2 = 0\n",
    "                while ingredient1 == ingredient2:\n",
    "                    ingredient1 = random.choice(drugspace)\n",
    "                    ingredient2 = random.choice(drugspace)\n",
    "                if ingredient2 in combo_dict[ingredient1] or ingredient1 in combo_dict[ingredient2]:\n",
    "                    continue\n",
    "                if [ingredient1, ingredient2] not in n_combos and [ingredient2, ingredient1] not in n_combos:\n",
    "                    n_combos.append([ingredient1, ingredient2])\n",
    "                    if len(n_combos) % 1000 == 0:\n",
    "                        print(len(n_combos))\n",
    "\n",
    "            for i in range(0, len(n_combos)):\n",
    "                n_combos.append([n_combos[i][1], n_combos[i][0]])\n",
    "\n",
    "            print('constructing dataset')\n",
    "            table = []\n",
    "            for r in existing_combinations:\n",
    "                if existing_combinations.index(r) % 1000 == 0:\n",
    "                    print(existing_combinations.index(r))\n",
    "                try:\n",
    "                    index1 = index_dict[r[0]]\n",
    "                    row1 = descriptors.loc[index1].drop('IDs').to_list()\n",
    "\n",
    "                    tarindex1 = tar_dict[r[0]]\n",
    "                    tarrow1 = targets.loc[tarindex1].drop('IDs').to_list()\n",
    "\n",
    "                    index2 = index_dict[r[1]]\n",
    "                    row2 = descriptors.loc[index2].drop('IDs').to_list()\n",
    "\n",
    "                    tarindex2 = tar_dict[r[1]]\n",
    "                    tarrow2 = targets.loc[tarindex1].drop('IDs').to_list()\n",
    "\n",
    "                    row = row1 + tarrow1 + row2 + tarrow2\n",
    "                    # row = row1 + row2\n",
    "                    # row = tarrow1 + tarrow2\n",
    "                    table.append(row)\n",
    "                except KeyboardInterrupt:\n",
    "                    raise\n",
    "                except:\n",
    "                    del existing_combinations[existing_combinations.index(r)]\n",
    "                    continue\n",
    "\n",
    "            data = np.array(table)\n",
    "\n",
    "            del table\n",
    "\n",
    "            results = []\n",
    "            for r in range(0, len(data)):\n",
    "                results.append(1)\n",
    "\n",
    "            table = []\n",
    "            for r in n_combos:\n",
    "                if n_combos.index(r) % 1000 == 0:\n",
    "                    print(n_combos.index(r))\n",
    "                try:\n",
    "                    index1 = index_dict[r[0]]\n",
    "                    row1 = descriptors.loc[index1].drop('IDs').to_list()\n",
    "\n",
    "                    tarindex1 = tar_dict[r[0]]\n",
    "                    tarrow1 = targets.loc[tarindex1].drop('IDs').to_list()\n",
    "\n",
    "                    index2 = index_dict[r[1]]\n",
    "                    row2 = descriptors.loc[index2].drop('IDs').to_list()\n",
    "\n",
    "                    tarindex2 = tar_dict[r[1]]\n",
    "                    tarrow2 = targets.loc[tarindex1].drop('IDs').to_list()\n",
    "\n",
    "                    row = row1 + tarrow1 + row2 + tarrow2\n",
    "                    # row = row1 + row2\n",
    "                    # row = tarrow1 + tarrow2\n",
    "                    table.append(row)\n",
    "                except KeyboardInterrupt:\n",
    "                    raise\n",
    "                except:\n",
    "                    del n_combos[n_combos.index(r)]\n",
    "                    continue\n",
    "\n",
    "            data1 = np.array(table)\n",
    "\n",
    "            del table\n",
    "\n",
    "            results1 = []\n",
    "            for r in range(0, len(data1)):\n",
    "                results1.append(0)\n",
    "\n",
    "            file = np.concatenate((data, data1), axis=0)\n",
    "            results = results + results1\n",
    "            del data, data1\n",
    "            print('dataset shape')\n",
    "            print(file.shape)\n",
    "\n",
    "            file = pd.DataFrame(file, columns=col_names)\n",
    "\n",
    "            scale_names = descriptors.columns.to_list()[1:]\n",
    "            scale_names = [x for x in scale_names if x in file.columns]\n",
    "            temp = [x + ' - 1' for x in scale_names]\n",
    "            scale_names = scale_names + temp\n",
    "\n",
    "            x_train, x_val, y_train, y_val = train_test_split(file, results, test_size=0.2)\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "\n",
    "            standard_transformer = Pipeline(steps=[\n",
    "                    ('standard', scaler)])\n",
    "\n",
    "            preprocessor = ColumnTransformer(\n",
    "                        remainder='passthrough', transformers=[\n",
    "                            ('std', standard_transformer , scale_names),\n",
    "                        ])\n",
    "            x_train = preprocessor.fit_transform(x_train)\n",
    "            x_val = preprocessor.transform(x_val)\n",
    "\n",
    "            x_train = x_train.astype('float32')\n",
    "            y_train = to_categorical(y_train)\n",
    "\n",
    "            print('training model')\n",
    "            model = tf.keras.models.Sequential()\n",
    "            n_cols = x_train.shape[1]\n",
    "            model.add(tf.keras.layers.Dense(n_cols, activation='relu', input_shape=(n_cols,)))\n",
    "            model.add(Dropout(0.2))\n",
    "            model.add(tf.keras.layers.Dense(int(n_cols / 2), activation='relu'))\n",
    "            model.add(Dropout(0.5))\n",
    "            model.add(tf.keras.layers.Dense(2, activation='linear'))\n",
    "\n",
    "            model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.0005, momentum=0.5), \n",
    "                      loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                     metrics=[tf.keras.metrics.AUC(), 'accuracy', tf.keras.metrics.Precision()])\n",
    "\n",
    "            early_stopping_monitor = EarlyStopping(patience=3)\n",
    "            model.fit(x_train, y_train, validation_data=(x_val, to_categorical(y_val)), epochs=100, batch_size = 32, callbacks=[early_stopping_monitor])\n",
    "\n",
    "            temp = tf.Variable(initial_value=1.0, trainable=True)\n",
    "            y_pred = model.predict(x_val)\n",
    "            y_test = to_categorical(y_val)\n",
    "\n",
    "            def compute_loss():\n",
    "                y_pred_model_w_temp = tf.math.divide(y_pred, temp)\n",
    "                loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(tf.convert_to_tensor(y_test), y_pred_model_w_temp))\n",
    "                return loss\n",
    "\n",
    "            optimizer = tf.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "            print('Temperature Initial value: {}'.format(temp.numpy()))\n",
    "\n",
    "            for i in range(300):\n",
    "                 opts = optimizer.minimize(compute_loss, var_list=[temp])\n",
    "\n",
    "            print('Temperature Final value: {}'.format(temp.numpy()))\n",
    "\n",
    "            if c == 2:\n",
    "                model2s.append([model, preprocessor, temp.numpy()])\n",
    "            else:\n",
    "                model4s.append([model, preprocessor, temp.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea58d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validation_set(drugspace):\n",
    "    combo_dict = {}\n",
    "    for i in drugspace:\n",
    "        combo_dict[i] = []\n",
    "    for i in combos:\n",
    "        try:\n",
    "            x = combo_dict[i[0]]\n",
    "            x = combo_dict[i[1]]\n",
    "            combo_dict[i[0]].append(i[1])\n",
    "            combo_dict[i[1]].append(i[0])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print('setting positive combos')\n",
    "    existing_combinations = []\n",
    "    for i in drugspace:\n",
    "        ingredients = combo_dict[i]\n",
    "        for i1 in ingredients:\n",
    "            existing_combinations.append([i, i1])\n",
    "\n",
    "    existing_combinations = list(map(list, set(map(frozenset, existing_combinations))))\n",
    "\n",
    "    n_combos = []\n",
    "    count = len(existing_combinations)\n",
    "    while len(n_combos) < count:\n",
    "        ingredient1 = 0\n",
    "        ingredient2 = 0\n",
    "        while ingredient1 == ingredient2:\n",
    "            ingredient1 = random.choice(drugspace)\n",
    "            ingredient2 = random.choice(drugspace)\n",
    "        if ingredient2 in combo_dict[ingredient1] or ingredient1 in combo_dict[ingredient2]:\n",
    "            continue\n",
    "        if [ingredient1, ingredient2] not in n_combos and [ingredient2, ingredient1] not in n_combos:\n",
    "            n_combos.append([ingredient1, ingredient2])\n",
    "            if len(n_combos) % 1000 == 0:\n",
    "                print(len(n_combos))\n",
    "\n",
    "    print('constructing dataset')\n",
    "    table = []\n",
    "    for r in existing_combinations:\n",
    "        if existing_combinations.index(r) % 1000 == 0:\n",
    "            print(existing_combinations.index(r))\n",
    "        try:\n",
    "            index1 = index_dict[r[0]]\n",
    "            row1 = descriptors.loc[index1].drop('IDs').to_list()\n",
    "\n",
    "            tarindex1 = tar_dict[r[0]]\n",
    "            tarrow1 = targets.loc[tarindex1].drop('IDs').to_list()\n",
    "\n",
    "            index2 = index_dict[r[1]]\n",
    "            row2 = descriptors.loc[index2].drop('IDs').to_list()\n",
    "\n",
    "            tarindex2 = tar_dict[r[1]]\n",
    "            tarrow2 = targets.loc[tarindex1].drop('IDs').to_list()\n",
    "\n",
    "            row = row1 + tarrow1 + row2 + tarrow2\n",
    "            # row = row1 + row2\n",
    "            # row = tarrow1 + tarrow2\n",
    "            table.append(row)\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            del existing_combinations[existing_combinations.index(r)]\n",
    "            continue\n",
    "\n",
    "    data = np.array(table)\n",
    "\n",
    "    del table\n",
    "\n",
    "    results = []\n",
    "    for r in range(0, len(data)):\n",
    "        results.append(1)\n",
    "\n",
    "    table = []\n",
    "    for r in n_combos:\n",
    "        if n_combos.index(r) % 1000 == 0:\n",
    "            print(n_combos.index(r))\n",
    "        try:\n",
    "            index1 = index_dict[r[0]]\n",
    "            row1 = descriptors.loc[index1].drop('IDs').to_list()\n",
    "\n",
    "            tarindex1 = tar_dict[r[0]]\n",
    "            tarrow1 = targets.loc[tarindex1].drop('IDs').to_list()\n",
    "\n",
    "            index2 = index_dict[r[1]]\n",
    "            row2 = descriptors.loc[index2].drop('IDs').to_list()\n",
    "\n",
    "            tarindex2 = tar_dict[r[1]]\n",
    "            tarrow2 = targets.loc[tarindex1].drop('IDs').to_list()\n",
    "\n",
    "            row = row1 + tarrow1 + row2 + tarrow2\n",
    "            # row = row1 + row2\n",
    "            # row = tarrow1 + tarrow2\n",
    "            table.append(row)\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            del n_combos[n_combos.index(r)]\n",
    "            continue\n",
    "\n",
    "    data1 = np.array(table)\n",
    "\n",
    "    del table\n",
    "\n",
    "    results1 = []\n",
    "    for r in range(0, len(data1)):\n",
    "        results1.append(0)\n",
    "\n",
    "    file = np.concatenate((data, data1), axis=0)\n",
    "    results = results + results1\n",
    "    del data, data1\n",
    "    print('dataset shape:')\n",
    "    print(file.shape)\n",
    "\n",
    "    val_set = pd.DataFrame(file, columns=col_names)\n",
    "    val_results = results\n",
    "    val_combos = existing_combinations + n_combos\n",
    "\n",
    "    return val_set, val_results, val_combos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
