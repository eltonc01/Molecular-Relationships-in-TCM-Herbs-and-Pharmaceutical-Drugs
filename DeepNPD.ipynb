{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee05799",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, recall_score, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from IPython.utils import io\n",
    "import time\n",
    "import joblib\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow_addons as tfa\n",
    "import ast\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import DataStructs\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e547bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain datasets\n",
    "\n",
    "df_combos = pd.read_csv('herb synergy combos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed340e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_herbs = pd.read_csv('updated herbs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb2ed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ingredients = pd.read_csv('cancer ingredients.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5add254",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = pd.read_csv('herb targets.csv')\n",
    "var = VarianceThreshold(0.02)\n",
    "var.fit_transform(targets.drop(columns='IDs')).shape[1]\n",
    "cols = var.get_feature_names_out(input_features=targets.drop(columns='IDs').columns)\n",
    "\n",
    "drop = []\n",
    "for i in targets.drop(columns='IDs').columns:\n",
    "    if i not in cols:\n",
    "        drop.append(i)\n",
    "targets = targets.drop(columns=drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d011358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors = pd.read_csv('cancer ingredients mordred descriptors + morgan fp dropped columns.csv')\n",
    "var = VarianceThreshold(0.02)\n",
    "var.fit_transform(descriptors.drop(columns='IDs')).shape[1]\n",
    "cols = var.get_feature_names_out(input_features=descriptors.drop(columns='IDs').columns)\n",
    "\n",
    "drop = []\n",
    "for i in descriptors.drop(columns='IDs').columns:\n",
    "    if i not in cols:\n",
    "        drop.append(i)\n",
    "        \n",
    "descriptors = descriptors.drop(columns=drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5981b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = descriptors.columns.to_list()[1:] + targets.columns.to_list()[1:]\n",
    "temp = [x + ' - 1' for x in col_names]\n",
    "col_names = col_names + temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e642ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ingredients = [descriptors.loc[x, 'IDs'] for x in range(0, len(descriptors))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d9545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "combos = df_combos.apply(lambda row: [row['Drug 1']] + [row['Drug 2']], axis=1).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169a0fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891d390d",
   "metadata": {},
   "outputs": [],
   "source": [
    "herb_dict = {}\n",
    "for r in range(0, len(df_herbs)):\n",
    "    items = df_herbs.loc[r, 'Ingredients'].split(', ')\n",
    "    name = df_herbs.loc[r, 'id']\n",
    "    herb_dict[name] = items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4024c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredient_dict = {}\n",
    "for r in range(0, len(df_ingredients)):\n",
    "    items = df_ingredients.loc[r, 'Ingredient_Smile']\n",
    "    name = df_ingredients.loc[r, 'IDs']\n",
    "    ingredient_dict[name] = items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c824d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_dict = {}\n",
    "for r in range(0, len(descriptors)):\n",
    "    index_dict[descriptors.loc[r, 'IDs']] = r\n",
    "    \n",
    "tar_dict = {}\n",
    "for r in range(0, len(targets)):\n",
    "    tar_dict[targets.loc[r, 'IDs']] = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fde2c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set validation dataset\n",
    "\n",
    "val_drugspace = random.sample(all_ingredients, 500)\n",
    "drugspace = val_drugspace\n",
    "val_drugspace = drugspace\n",
    "\n",
    "combo_dict = {}\n",
    "for i in drugspace:\n",
    "    combo_dict[i] = []\n",
    "for i in combos:\n",
    "    try:\n",
    "        x = combo_dict[i[0]]\n",
    "        x = combo_dict[i[1]]\n",
    "        combo_dict[i[0]].append(i[1])\n",
    "        combo_dict[i[1]].append(i[0])\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print('setting positive combos')\n",
    "existing_combinations = []\n",
    "for i in drugspace:\n",
    "    ingredients = combo_dict[i]\n",
    "    for i1 in ingredients:\n",
    "        existing_combinations.append([i, i1])\n",
    "        \n",
    "existing_combinations = list(map(list, set(map(frozenset, existing_combinations))))\n",
    "\n",
    "drop = []\n",
    "for i in list(combo_dict):\n",
    "    if len(combo_dict[i]) == 0:\n",
    "        drop.append(i)\n",
    "\n",
    "drugspace = [x for x in drugspace if x not in drop]\n",
    "\n",
    "print('setting negative combos')\n",
    "n_combos = []\n",
    "count = len(existing_combinations)\n",
    "while len(n_combos) < count:\n",
    "    ingredient1 = 0\n",
    "    ingredient2 = 0\n",
    "    while ingredient1 == ingredient2:\n",
    "        ingredient1 = random.choice(drugspace)\n",
    "        ingredient2 = random.choice(drugspace)\n",
    "    if ingredient2 in combo_dict[ingredient1] or ingredient1 in combo_dict[ingredient2]:\n",
    "        continue\n",
    "    if [ingredient1, ingredient2] not in n_combos and [ingredient2, ingredient1] not in n_combos:\n",
    "        n_combos.append([ingredient1, ingredient2])\n",
    "        if len(n_combos) % 1000 == 0:\n",
    "            print(len(n_combos))\n",
    "                \n",
    "print('constructing dataset')\n",
    "table = []\n",
    "for r in existing_combinations:\n",
    "    if existing_combinations.index(r) % 1000 == 0:\n",
    "        print(existing_combinations.index(r))\n",
    "    try:\n",
    "        index1 = index_dict[r[0]]\n",
    "        row1 = descriptors.loc[index1].drop('IDs').to_list()\n",
    "        \n",
    "        tarindex1 = tar_dict[r[0]]\n",
    "        tarrow1 = targets.loc[tarindex1].drop('IDs').to_list()\n",
    "        \n",
    "        index2 = index_dict[r[1]]\n",
    "        row2 = descriptors.loc[index2].drop('IDs').to_list()\n",
    "        \n",
    "        tarindex2 = tar_dict[r[1]]\n",
    "        tarrow2 = targets.loc[tarindex1].drop('IDs').to_list()\n",
    "        \n",
    "        row = row1 + tarrow1 + row2 + tarrow2\n",
    "        # row = row1 + row2\n",
    "        # row = tarrow1 + tarrow2\n",
    "        table.append(row)\n",
    "    except KeyboardInterrupt:\n",
    "        raise\n",
    "    except:\n",
    "        del existing_combinations[existing_combinations.index(r)]\n",
    "        continue\n",
    "    \n",
    "data = np.array(table)\n",
    "\n",
    "del table\n",
    "\n",
    "results = []\n",
    "for r in range(0, len(data)):\n",
    "    results.append(1)\n",
    "    \n",
    "table = []\n",
    "for r in n_combos:\n",
    "    if n_combos.index(r) % 1000 == 0:\n",
    "        print(n_combos.index(r))\n",
    "    try:\n",
    "        index1 = index_dict[r[0]]\n",
    "        row1 = descriptors.loc[index1].drop('IDs').to_list()\n",
    "        \n",
    "        tarindex1 = tar_dict[r[0]]\n",
    "        tarrow1 = targets.loc[tarindex1].drop('IDs').to_list()\n",
    "        \n",
    "        index2 = index_dict[r[1]]\n",
    "        row2 = descriptors.loc[index2].drop('IDs').to_list()\n",
    "        \n",
    "        tarindex2 = tar_dict[r[1]]\n",
    "        tarrow2 = targets.loc[tarindex1].drop('IDs').to_list()\n",
    "        \n",
    "        row = row1 + tarrow1 + row2 + tarrow2\n",
    "        # row = row1 + row2\n",
    "        # row = tarrow1 + tarrow2\n",
    "        table.append(row)\n",
    "    except KeyboardInterrupt:\n",
    "        raise\n",
    "    except:\n",
    "        del n_combos[n_combos.index(r)]\n",
    "        continue\n",
    "    \n",
    "data1 = np.array(table)\n",
    "\n",
    "del table\n",
    "\n",
    "results1 = []\n",
    "for r in range(0, len(data1)):\n",
    "    results1.append(0)\n",
    "    \n",
    "file = np.concatenate((data, data1), axis=0)\n",
    "results = results + results1\n",
    "del data, data1\n",
    "print('dataset shape:')\n",
    "print(file.shape)\n",
    "\n",
    "val_set = pd.DataFrame(file, columns=col_names)\n",
    "val_results = results\n",
    "val_combos = existing_combinations + n_combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a059a887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conduct ensemble learning\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "fullspace = random.sample(all_ingredients, 1500 * 10)\n",
    "fullspace = list(chunks(fullspace, 1500))\n",
    "accuracies = []\n",
    "rocs = []\n",
    "models = {}\n",
    "scalers = {}\n",
    "end_drugspaces = []\n",
    "counters = []\n",
    "test_sets = {}\n",
    "validation_pred= {}\n",
    "validation_y = {}\n",
    "\n",
    "for i in fullspace[:2]:\n",
    "    index = fullspace.index(i)\n",
    "    print(f'iteration: {index + 1}')\n",
    "    drugspace = i\n",
    "    combo_dict = {}\n",
    "    for i in drugspace:\n",
    "        combo_dict[i] = []\n",
    "    for i in combos:\n",
    "        try:\n",
    "            x = combo_dict[i[0]]\n",
    "            x = combo_dict[i[1]]\n",
    "            combo_dict[i[0]].append(i[1])\n",
    "            combo_dict[i[1]].append(i[0])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print('setting positive combos')\n",
    "    existing_combinations = []\n",
    "    for i in drugspace:\n",
    "        ingredients = combo_dict[i]\n",
    "        for i1 in ingredients:\n",
    "            existing_combinations.append([i, i1])\n",
    "\n",
    "    existing_combinations = list(map(list, set(map(frozenset, existing_combinations))))\n",
    "        \n",
    "    count = []\n",
    "    drop = []\n",
    "    for i in list(combo_dict):\n",
    "        if len(combo_dict[i]) == 0:\n",
    "            drop.append(i)\n",
    "            continue\n",
    "        for i1 in range(0, len(combo_dict[i])):\n",
    "            count.append(i)\n",
    "\n",
    "    drugspace = [x for x in drugspace if x not in drop]\n",
    "    \n",
    "    end_drugspaces.append(drugspace)\n",
    "    \n",
    "    counters.append(Counter(count))\n",
    "        \n",
    "    print('setting negative combos')\n",
    "    n_combos = []\n",
    "    count = len(existing_combinations)\n",
    "    while len(n_combos) < count:\n",
    "        ingredient1 = 0\n",
    "        ingredient2 = 0\n",
    "        while ingredient1 == ingredient2:\n",
    "            ingredient1 = random.choice(drugspace)\n",
    "            ingredient2 = random.choice(drugspace)\n",
    "        if ingredient2 in combo_dict[ingredient1] or ingredient1 in combo_dict[ingredient2]:\n",
    "            continue\n",
    "        if [ingredient1, ingredient2] not in n_combos and [ingredient2, ingredient1] not in n_combos:\n",
    "            n_combos.append([ingredient1, ingredient2])\n",
    "            if len(n_combos) % 1000 == 0:\n",
    "                print(len(n_combos))\n",
    "        \n",
    "    print('constructing dataset')\n",
    "    table = []\n",
    "    for r in existing_combinations:\n",
    "        if existing_combinations.index(r) % 1000 == 0:\n",
    "            print(existing_combinations.index(r))\n",
    "        try:\n",
    "            index1 = index_dict[r[0]]\n",
    "            row1 = descriptors.loc[index1].drop('IDs').to_list()\n",
    "\n",
    "            tarindex1 = tar_dict[r[0]]\n",
    "            tarrow1 = targets.loc[tarindex1].drop('IDs').to_list()\n",
    "\n",
    "            index2 = index_dict[r[1]]\n",
    "            row2 = descriptors.loc[index2].drop('IDs').to_list()\n",
    "\n",
    "            tarindex2 = tar_dict[r[1]]\n",
    "            tarrow2 = targets.loc[tarindex1].drop('IDs').to_list()\n",
    "\n",
    "            row = row1 + tarrow1 + row2 + tarrow2\n",
    "            # row = row1 + row2\n",
    "            # row = tarrow1 + tarrow2\n",
    "            table.append(row)\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            del existing_combinations[existing_combinations.index(r)]\n",
    "            continue\n",
    "\n",
    "    data = np.array(table)\n",
    "\n",
    "    del table\n",
    "\n",
    "    results = []\n",
    "    for r in range(0, len(data)):\n",
    "        results.append(1)\n",
    "\n",
    "    table = []\n",
    "    for r in n_combos:\n",
    "        if n_combos.index(r) % 1000 == 0:\n",
    "            print(n_combos.index(r))\n",
    "        try:\n",
    "            index1 = index_dict[r[0]]\n",
    "            row1 = descriptors.loc[index1].drop('IDs').to_list()\n",
    "\n",
    "            tarindex1 = tar_dict[r[0]]\n",
    "            tarrow1 = targets.loc[tarindex1].drop('IDs').to_list()\n",
    "\n",
    "            index2 = index_dict[r[1]]\n",
    "            row2 = descriptors.loc[index2].drop('IDs').to_list()\n",
    "\n",
    "            tarindex2 = tar_dict[r[1]]\n",
    "            tarrow2 = targets.loc[tarindex1].drop('IDs').to_list()\n",
    "\n",
    "            row = row1 + tarrow1 + row2 + tarrow2\n",
    "            # row = row1 + row2\n",
    "            # row = tarrow1 + tarrow2\n",
    "            table.append(row)\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            del n_combos[n_combos.index(r)]\n",
    "            continue\n",
    "\n",
    "    data1 = np.array(table)\n",
    "\n",
    "    del table\n",
    "\n",
    "    results1 = []\n",
    "    for r in range(0, len(data1)):\n",
    "        results1.append(0)\n",
    "        \n",
    "    file = np.concatenate((data, data1), axis=0)\n",
    "    results = results + results1\n",
    "    del data, data1\n",
    "    print('dataset shape')\n",
    "    print(file.shape)\n",
    "    \n",
    "    file = pd.DataFrame(file, columns=col_names)\n",
    "    \n",
    "    names = descriptors.columns.to_list()[1:] + targets.columns.to_list()[1:]\n",
    "\n",
    "    drop = [x for x in file.columns if x in names]\n",
    "    temp1 = file.drop(columns=drop)\n",
    "    temp1.columns = drop\n",
    "    drop = [x for x in file.columns if x not in names]\n",
    "    temp2 = file.drop(columns=drop)\n",
    "    temp2.columns = drop\n",
    "\n",
    "    file = pd.concat([file, pd.concat([temp1, temp2], axis=1)]).reset_index(drop=True)\n",
    "    del temp1, temp2\n",
    "    results = results + results\n",
    "    \n",
    "    scale_names = descriptors.columns.to_list()[1:]\n",
    "    scale_names = [x for x in scale_names if x in file.columns]\n",
    "    temp = [x + ' - 1' for x in scale_names]\n",
    "    scale_names = scale_names + temp\n",
    "    \n",
    "    x_train, y_train = shuffle(file, results)\n",
    "    del file\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    standard_transformer = Pipeline(steps=[\n",
    "            ('standard', scaler)])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "            remainder='passthrough',\n",
    "            transformers=[('std', standard_transformer , scale_names),])\n",
    "    x_train = preprocessor.fit_transform(x_train)\n",
    "    \n",
    "    scalers['scaler' + str(index)] = preprocessor\n",
    "        \n",
    "    x_train = x_train.astype('float32')\n",
    "    y_train = to_categorical(y_train)\n",
    "    \n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2)\n",
    "        \n",
    "    print('training model')\n",
    "    model = tf.keras.models.Sequential()\n",
    "    n_cols = x_train.shape[1]\n",
    "    model.add(tf.keras.layers.Dense(n_cols, activation='relu', input_shape=(n_cols,)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(tf.keras.layers.Dense(int(n_cols / 2), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    # model.add(tf.keras.layers.Dense(2, activation='linear'))\n",
    "    model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.0005, momentum=0.5), \n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "             metrics=[tf.keras.metrics.AUC(), 'accuracy', tf.keras.metrics.Precision()])\n",
    "    \n",
    "    early_stopping_monitor = EarlyStopping(patience=3)\n",
    "    model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=100, batch_size = 32, callbacks=[early_stopping_monitor])\n",
    "    \n",
    "    print('iteration finished')\n",
    "    model.save(f'herb ensemble models/individual target models/softmax models/model{str(index)}.h5')\n",
    "    joblib.dump(preprocessor, f'herb ensemble models/individual target models/softmax models/scaler{str(index)}.save')\n",
    "    x_val = pd.DataFrame(x_val, columns=col_names)\n",
    "    y = []\n",
    "    for i1 in y_val:\n",
    "        if i1[0] == 1:\n",
    "            y.append(0)\n",
    "        else:\n",
    "            y.append(1)\n",
    "    x_val.insert(0, 'Results', y)\n",
    "    x_val.to_csv(f'herb ensemble models/individual target models/softmax models/set{str(index)}.csv', index=False)\n",
    "\n",
    "    del model\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2ddbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp scaling code obtained here: https://sourajit16-02-93.medium.com/neural-network-calibration-46997f8c872c\n",
    "\n",
    "temp = tf.Variable(initial_value=1.0, trainable=True)\n",
    "\n",
    "def compute_loss():\n",
    "    y_pred_model_w_temp = tf.math.divide(y_pred, temp)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(tf.convert_to_tensor(keras.utils.to_categorical(Y)), y_pred_model_w_temp))\n",
    "    return loss\n",
    "\n",
    "optimizer = tf.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "print(‘Temperature Initial value: {}’.format(temp.numpy()))\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "     opts = optimizer.minimize(compute_loss, var_list=[temp])\n",
    "        \n",
    "print(‘Temperature Final value: {}’.format(temp.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937441be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, precision_score, recall_score, roc_curve, auc, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e823169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on test sets from saved models\n",
    "\n",
    "acc = []\n",
    "roc = []\n",
    "prc = []\n",
    "prec = []\n",
    "f1 = []\n",
    "rec = []\n",
    "spec = []\n",
    "curve = []\n",
    "prc_curve = []\n",
    "\n",
    "for i in range(0, 10):\n",
    "    test_set = pd.read_csv(f'herb ensemble models/individual target models/test sets/set{i}.csv')\n",
    "    model = tf.keras.models.load_model(f'herb ensemble models/individual target models/model{i}.h5')\n",
    "    scaler = scalers[i]\n",
    "    x = test_set.drop(columns='Results')\n",
    "    y = test_set['Results'].to_list()\n",
    "    temp = variables[i]\n",
    "    \n",
    "    logits = model.predict(x)\n",
    "    predictions = tf.nn.softmax(logits / temp)\n",
    "    \n",
    "    y_val = y\n",
    "    \n",
    "    binary_pred = []\n",
    "    for i in predictions:\n",
    "        if i[1] >= 0.5:\n",
    "            binary_pred.append(1)\n",
    "        else:\n",
    "            binary_pred.append(0)\n",
    "    \n",
    "    acc.append(accuracy_score(y_val, binary_pred))\n",
    "    f1.append(f1_score(y_val, binary_pred))\n",
    "    rec.append(recall_score(y_val, binary_pred))\n",
    "    prec.append(precision_score(y_val, binary_pred))\n",
    "    spec.append(recall_score(y_val, binary_pred, pos_label=0))\n",
    "        \n",
    "    fpr, tpr, thresholds = roc_curve(y_val, predictions[:,1])\n",
    "    curve.append([fpr, tpr, thresholds])\n",
    "    roc.append(auc(fpr, tpr))\n",
    "    precision, recall, thresholds = precision_recall_curve(y_val, predictions[:,1])\n",
    "    prc_curve.append([precision, recall, thresholds])\n",
    "    prc.append(auc(recall, precision))\n",
    "    \n",
    "    import gc\n",
    "    \n",
    "    del model\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5547e9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity based weight adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdce49e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_drugspace = []\n",
    "for i in fullspace:\n",
    "    combined_drugspace = combined_drugspace + i\n",
    "\n",
    "drugspace_smi = []\n",
    "for i in combined_drugspace:\n",
    "    drugspace_smi.append(ingredient_dict[i])\n",
    "\n",
    "mol2 = [Chem.MolFromSmiles(x) for x in drugspace_smi]\n",
    "mol2 = [AllChem.GetMorganFingerprintAsBitVect(x, 3, nBits=2048) for x in mol2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51370b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_dict_list = []\n",
    "dataset_sizes = []\n",
    "for drugspace in fullspace:\n",
    "    combo_dict = {}\n",
    "    for i in drugspace:\n",
    "        combo_dict[i] = []\n",
    "    for i in combos:\n",
    "        try:\n",
    "            x = combo_dict[i[0]]\n",
    "            x = combo_dict[i[1]]\n",
    "            combo_dict[i[0]].append(i[1])\n",
    "            combo_dict[i[1]].append(i[0])\n",
    "        except:\n",
    "            continue\n",
    "    size = 0\n",
    "    for i in list(combo_dict):\n",
    "        size += len(combo_dict[i])\n",
    "    combo_dict_list.append(combo_dict)\n",
    "    dataset_sizes.append(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3439b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate on unseen drugs, sbwa vs. equal weights\n",
    "\n",
    "acc = []\n",
    "roc = []\n",
    "prc = []\n",
    "prec = []\n",
    "f1 = []\n",
    "rec = []\n",
    "spec = []\n",
    "curve = []\n",
    "prc_curve = []\n",
    "\n",
    "acc1 = []\n",
    "roc1 = []\n",
    "prc1 = []\n",
    "prec1 = []\n",
    "f11 = []\n",
    "rec1 = []\n",
    "spec1 = []\n",
    "curve1 = []\n",
    "prc_curve1 = []\n",
    "\n",
    "for r in range(0, 15):\n",
    "    print(f'iteration: {r + 1} / 10')\n",
    "    val_index = r\n",
    "\n",
    "    drugspace = ast.literal_eval(df_models.loc[r, 'End Drugspace'])\n",
    "    val_set, val_results, val_combos = create_validation_set(drugspace)\n",
    "    similarity_dict = create_similarity_dict(drugspace)\n",
    "    similarity_list = create_similarity_list(val_combos, similarity_dict, val_index)\n",
    "        \n",
    "    models = []\n",
    "    for r in range(0, 8):\n",
    "        models.append(tf.keras.models.load_model(f'herb ensemble models/individual descriptor models/model{r}.h5'))\n",
    "    \n",
    "    all_predictions = []\n",
    "    for r in range(0, 8):\n",
    "        if r == val_index:\n",
    "            continue\n",
    "        temp_pred = tf.nn.softmax(models[r].predict(scalers[r].transform(val_set)) / variables[r])\n",
    "        all_predictions.append(temp_pred)\n",
    "        \n",
    "    del models\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    gc.collect()\n",
    "        \n",
    "    models = [None, None, None, None, None, None, None, None]\n",
    "    for r in range(8, 15):\n",
    "        models.append(tf.keras.models.load_model(f'herb ensemble models/individual descriptor models/model{r}.h5'))\n",
    "\n",
    "    for r in range(8, 15):\n",
    "        if r == val_index:\n",
    "            continue\n",
    "        temp_pred = tf.nn.softmax(models[r].predict(scalers[r].transform(val_set)) / variables[r])\n",
    "        all_predictions.append(temp_pred)\n",
    "        \n",
    "    del models\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    predictions = []\n",
    "    for r in range(0, len(val_set)):\n",
    "        if r % 1000 == 0:\n",
    "            print(r)\n",
    "        weights = [x[1] for x in similarity_list[r]]\n",
    "        pred_list = []\n",
    "        for i in all_predictions:\n",
    "            pred_list.append(i[r])\n",
    "        pred = np.average(np.array(pred_list), axis=0, weights=weights)\n",
    "        predictions.append(pred)\n",
    "        \n",
    "    del val_set\n",
    "        \n",
    "    predictions = np.array(predictions)\n",
    "        \n",
    "    binary_pred = []\n",
    "    for i in predictions:\n",
    "        if i[0] > i[1]:\n",
    "            binary_pred.append(0)\n",
    "        else:\n",
    "            binary_pred.append(1)\n",
    "            \n",
    "    y_val = val_results\n",
    "    \n",
    "    acc.append(accuracy_score(y_val, binary_pred))\n",
    "    f1.append(f1_score(y_val, binary_pred))\n",
    "    rec.append(recall_score(y_val, binary_pred))\n",
    "    prec.append(precision_score(y_val, binary_pred))\n",
    "    spec.append(recall_score(y_val, binary_pred, pos_label=0))\n",
    "        \n",
    "    fpr, tpr, thresholds = roc_curve(y_val, predictions[:,1])\n",
    "    curve.append([fpr, tpr, thresholds])\n",
    "    roc.append(auc(fpr, tpr))\n",
    "    precision, recall, thresholds = precision_recall_curve(y_val, predictions[:,1])\n",
    "    prc_curve.append([precision, recall, thresholds])\n",
    "    prc.append(auc(recall, precision))\n",
    "    \n",
    "    predictions = np.mean(np.array(all_predictions), axis=0)\n",
    "    \n",
    "    print(f'accuracy is {accuracy_score(y_val, binary_pred)}, precision is {precision_score(y_val, binary_pred)}, recall is {recall_score(y_val, binary_pred)}')\n",
    "    \n",
    "    binary_pred = []\n",
    "    for i in predictions:\n",
    "        if i[0] > i[1]:\n",
    "            binary_pred.append(0)\n",
    "        else:\n",
    "            binary_pred.append(1)\n",
    "            \n",
    "    y_val = val_results\n",
    "    \n",
    "    acc1.append(accuracy_score(y_val, binary_pred))\n",
    "    f11.append(f1_score(y_val, binary_pred))\n",
    "    rec1.append(recall_score(y_val, binary_pred))\n",
    "    prec1.append(precision_score(y_val, binary_pred))\n",
    "    spec1.append(recall_score(y_val, binary_pred, pos_label=0))\n",
    "        \n",
    "    fpr, tpr, thresholds = roc_curve(y_val, predictions[:,1])\n",
    "    curve1.append([fpr, tpr, thresholds])\n",
    "    roc1.append(auc(fpr, tpr))\n",
    "    precision, recall, thresholds = precision_recall_curve(y_val, predictions[:,1])\n",
    "    prc_curve1.append([precision, recall, thresholds])\n",
    "    prc1.append(auc(recall, precision))\n",
    "    \n",
    "    print(f'accuracy is {accuracy_score(y_val, binary_pred)}, precision is {precision_score(y_val, binary_pred)}, recall is {recall_score(y_val, binary_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005ad4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sbwa classes\n",
    "\n",
    "def create_similarity_dict(val_drugspace):\n",
    "    similarity_dict = {}\n",
    "    for drug in val_drugspace:\n",
    "        smi = ingredient_dict[drug]\n",
    "        mol1 = Chem.MolFromSmiles(smi)\n",
    "        mol1 = AllChem.GetMorganFingerprintAsBitVect(mol1, 3, nBits=2048)\n",
    "        s = DataStructs.BulkTanimotoSimilarity(mol1, mol2)\n",
    "        similarity = [[combined_drugspace[x], s[x]] for x in range(0, len(s))]\n",
    "\n",
    "        similarity.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        similar_molecules = []\n",
    "        for i in similarity:\n",
    "            if i[1] >= 0.70:\n",
    "                similar_molecules.append(i)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        similarity_dict[drug] = similar_molecules\n",
    "        \n",
    "    return similarity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa65bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_similarity_list(val_combos, similarity_dict, val_index):\n",
    "    similarity_list = []\n",
    "\n",
    "    for i in val_combos:\n",
    "\n",
    "        similar_molecules = similarity_dict[i[0]] + similarity_dict[i[1]]\n",
    "\n",
    "        similar_models = []\n",
    "        for i in fullspace:\n",
    "            if fullspace.index(i) == val_index:\n",
    "                continue\n",
    "            temp = []\n",
    "            found = False\n",
    "            for i1 in similar_molecules:\n",
    "                if i1[0] in i:\n",
    "                    found = True\n",
    "                    temp.append(i1)\n",
    "            if found is True:\n",
    "                similar_models.append([fullspace.index(i), temp])\n",
    "            else:\n",
    "                similar_models.append([fullspace.index(i), None])\n",
    "\n",
    "        weights = []\n",
    "        for r in range(0, 9):\n",
    "            i = similar_models[r]\n",
    "            if pd.isnull(i[1]) is True:\n",
    "                weights.append([i[0], 1 / 9])\n",
    "                continue\n",
    "            scores = [x[1] for x in i[1]]\n",
    "            s = np.array(scores).mean()\n",
    "            temp_dict = combo_dict_list[i[0]]\n",
    "            n = 0\n",
    "            for i1 in i[1]:\n",
    "                n += len(temp_dict[i1[0]])\n",
    "            d = dataset_sizes[i[0]]\n",
    "            weight = s * (n / (0.001 * d)) + (1 / 9)\n",
    "            weights.append([i[0], weight])\n",
    "\n",
    "        sums = [x[1] for x in weights]\n",
    "        multiplier = 100 / sum(sums) * 0.01\n",
    "\n",
    "        weights = [[x[0], x[1] * multiplier] for x in weights]\n",
    "\n",
    "        similarity_list.append(weights)\n",
    "        \n",
    "    return similarity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3085cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validation_set(drugspace):\n",
    "    combo_dict = {}\n",
    "    for i in drugspace:\n",
    "        combo_dict[i] = []\n",
    "    for i in combos:\n",
    "        try:\n",
    "            x = combo_dict[i[0]]\n",
    "            x = combo_dict[i[1]]\n",
    "            combo_dict[i[0]].append(i[1])\n",
    "            combo_dict[i[1]].append(i[0])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print('setting positive combos')\n",
    "    existing_combinations = []\n",
    "    for i in drugspace:\n",
    "        ingredients = combo_dict[i]\n",
    "        for i1 in ingredients:\n",
    "            existing_combinations.append([i, i1])\n",
    "\n",
    "    existing_combinations = list(map(list, set(map(frozenset, existing_combinations))))\n",
    "\n",
    "    drop = []\n",
    "    for i in list(combo_dict):\n",
    "        if len(combo_dict[i]) == 0:\n",
    "            drop.append(i)\n",
    "\n",
    "    drugspace = [x for x in drugspace if x not in drop]\n",
    "\n",
    "    print('setting negative combos')\n",
    "    n_combos = []\n",
    "    count = len(existing_combinations)\n",
    "    while len(n_combos) < count:\n",
    "        ingredient1 = 0\n",
    "        ingredient2 = 0\n",
    "        while ingredient1 == ingredient2:\n",
    "            ingredient1 = random.choice(drugspace)\n",
    "            ingredient2 = random.choice(drugspace)\n",
    "        if ingredient2 in combo_dict[ingredient1] or ingredient1 in combo_dict[ingredient2]:\n",
    "            continue\n",
    "        if [ingredient1, ingredient2] not in n_combos and [ingredient2, ingredient1] not in n_combos:\n",
    "            n_combos.append([ingredient1, ingredient2])\n",
    "            if len(n_combos) % 1000 == 0:\n",
    "                print(len(n_combos))\n",
    "\n",
    "\n",
    "    print('constructing dataset')\n",
    "    table = []\n",
    "    for r in existing_combinations:\n",
    "        if existing_combinations.index(r) % 1000 == 0:\n",
    "            print(existing_combinations.index(r))\n",
    "        try:\n",
    "            index1 = index_dict[r[0]]\n",
    "            row1 = descriptors.loc[index1].drop('IDs').to_list()\n",
    "\n",
    "            tarindex1 = tar_dict[r[0]]\n",
    "            tarrow1 = targets.loc[tarindex1].drop('IDs').to_list()\n",
    "\n",
    "            index2 = index_dict[r[1]]\n",
    "            row2 = descriptors.loc[index2].drop('IDs').to_list()\n",
    "\n",
    "            tarindex2 = tar_dict[r[1]]\n",
    "            tarrow2 = targets.loc[tarindex1].drop('IDs').to_list()\n",
    "\n",
    "            row = row1 + tarrow1 + row2 + tarrow2\n",
    "            # row = row1 + row2\n",
    "            # row = tarrow1 + tarrow2\n",
    "            table.append(row)\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            del existing_combinations[existing_combinations.index(r)]\n",
    "            continue\n",
    "\n",
    "    data = np.array(table)\n",
    "\n",
    "    del table\n",
    "\n",
    "    results = []\n",
    "    for r in range(0, len(data)):\n",
    "        results.append(1)\n",
    "\n",
    "    table = []\n",
    "    for r in n_combos:\n",
    "        if n_combos.index(r) % 1000 == 0:\n",
    "            print(n_combos.index(r))\n",
    "        try:\n",
    "            index1 = index_dict[r[0]]\n",
    "            row1 = descriptors.loc[index1].drop('IDs').to_list()\n",
    "\n",
    "            tarindex1 = tar_dict[r[0]]\n",
    "            tarrow1 = targets.loc[tarindex1].drop('IDs').to_list()\n",
    "\n",
    "            index2 = index_dict[r[1]]\n",
    "            row2 = descriptors.loc[index2].drop('IDs').to_list()\n",
    "\n",
    "            tarindex2 = tar_dict[r[1]]\n",
    "            tarrow2 = targets.loc[tarindex1].drop('IDs').to_list()\n",
    "\n",
    "            row = row1 + tarrow1 + row2 + tarrow2\n",
    "            # row = row1 + row2\n",
    "            # row = tarrow1 + tarrow2\n",
    "            table.append(row)\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            del n_combos[n_combos.index(r)]\n",
    "            continue\n",
    "\n",
    "    data1 = np.array(table)\n",
    "\n",
    "    del table\n",
    "\n",
    "    results1 = []\n",
    "    for r in range(0, len(data1)):\n",
    "        results1.append(0)\n",
    "\n",
    "    file = np.concatenate((data, data1), axis=0)\n",
    "    results = results + results1\n",
    "    del data, data1\n",
    "    print('dataset shape:')\n",
    "    print(file.shape)\n",
    "\n",
    "    val_set = pd.DataFrame(file, columns=col_names)\n",
    "    val_results = results\n",
    "    val_combos = existing_combinations + n_combos\n",
    "    \n",
    "    return val_set, val_results, val_combos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
